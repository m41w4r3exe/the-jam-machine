{"cells":[{"cell_type":"markdown","metadata":{"id":"I3vAEjWWI2On"},"source":["# The Jam Machine \n","\n","## decription of the model here\n","### - Sequence length 1024\n","### - whole dataset\n","### - tristan's encoder\n","### - improved features\n","    - pushing on Hugging face\n","    - writing the tokenized dataset on disk\n","    - loading the tokenized dataset --> saves times for big datasets\n"]},{"cell_type":"markdown","metadata":{"id":"sy73jjVLBK6K"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28882,"status":"ok","timestamp":1670279228164,"user":{"displayName":"Jean Simonnet","userId":"17244487804686370097"},"user_tz":-60},"id":"Nj3_dbPjL0tB","outputId":"89edf1a1-f5a4-4d6e-b82e-d709e32d4d7f"},"outputs":[],"source":["## THIS NEEDS TO BE CLEANED - NOT EVERYTHING IS NECESSARY\n","# basic packages requirements\n","!pip install wandb\n","import os\n","from pathlib import Path\n","import glob\n","import random\n","import shutil\n","import tqdm\n","import matplotlib.pyplot as plt\n","import typing\n","import json\n","# tensorflow\n","# import tensorflow as tf\n","# from tensorflow.keras import preprocessing\n","# from tensorflow.keras import models, layers\n","# tf.config.list_physical_devices(\"GPU\")\n","# transformers\n","!pip install transformers tokenizers\n","from transformers import PreTrainedTokenizerFast\n","from transformers import DataCollatorForLanguageModeling\n","from transformers import GPT2Config, GPT2LMHeadModel\n","from transformers import TrainingArguments, Trainer\n","# tokenizer\n","from tokenizers import Tokenizer\n","from tokenizers.models import BPE\n","from tokenizers.trainers import BpeTrainer\n","from tokenizers.pre_tokenizers import Whitespace\n","# torch\n","import torch\n","\n","# install requirements\n","# !pipenv lock -r > requirements.txt\n","# !pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"Z-1i_bmABXCJ"},"source":["# Setup requirements\n","Here you should set the path for saving your trained model etc"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5534,"status":"ok","timestamp":1670279381670,"user":{"displayName":"Jean Simonnet","userId":"17244487804686370097"},"user_tz":-60},"id":"r3sLiMIs8If3","outputId":"4c7c397b-fe86-42a8-84fc-ceccd4c38375"},"outputs":[],"source":["!git clone https://github.com/m41w4r3exe/the-jam-machine.git\n","\n","# move into the git folder to have all scripts in the local folder\n","os.chdir(\"./the-jam-machine\")\n","\n","# check that we re in the right happy place\n","print(f\"Current working directory: {os.getcwd()}\")\n","print(f\"Files and Folders in directory:\")\n","os.listdir()\n","\n","# mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# define path\n","drive_path = Path(\"/content/drive/MyDrive/the_jam_machine/\")\n","# midi_path = Path(f\"{drive_path}/midi_data\")\n","# encoded_midi_path = Path(f\"{drive_path}/midi_encoded\")\n","\n","# DATA\n","encoded_midi_path = Path(f\"{drive_path}/midi_encoded_by_tristan\")\n","\n","# TOKENIZER\n","tokenizer_path = Path(f\"{drive_path}/tokenizer\")\n","tokenizer_file = \"tokenizer.json\"\n","load_tokenizer_pretrained = True\n","load_tokenized_datasets = True\n","\n","# MODEL\n","model_path = Path(f\"{drive_path}/model_jean_1024_8layers\")\n","if not os.path.exists(model_path):\n","  print(f\"creating {model_path}\")\n","  os.mkdir(model_path)\n","\n","# HuggingFace repo\n","hf_repo = \"misnaej/the-jam-machine-1024\"\n","hf_token = \n","\n","# device = \"cpu\"\n","device = \"cuda\" \n","train_from_scratch = False\n","\n","if train_from_scratch:\n","  train_from_this_checkpoint = None\n","\n","elif not train_from_scratch:\n","  train_from_this_checkpoint = \"checkpoint-86906\"\n","  additionnal_epochs_to_run = 0\n","  checkpoint_path = Path(f\"{model_path}/{train_from_this_checkpoint}\")\n"]},{"cell_type":"markdown","metadata":{"id":"5gwWdCMCeLPS"},"source":["# Encoding Midi into Text (Done already)\n","from `mymusic.mid` files from `midi_path` to `music.txt` files in `encoded_midi_path`"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1670279261157,"user":{"displayName":"Jean Simonnet","userId":"17244487804686370097"},"user_tz":-60},"id":"IX9A654ucg-u"},"outputs":[],"source":["#@title File encoding\n","#!python encoder_mlike.py --midi_path {midi_path} --encoded_midi_path {encoded_midi_path}"]},{"cell_type":"markdown","metadata":{"id":"rpXIJDq8dyf3"},"source":["# Dataset\n","- load the encoded data\n","- training set - ?? % (done already by Tristan's encoding)\n","- validation set - ?? % (done already by Tristan's encoding)\n","\n","- Tokenizing\n"]},{"cell_type":"markdown","metadata":{"id":"8_0WBWxkLrMu"},"source":["## Loading the data"]},{"cell_type":"markdown","metadata":{"id":"81ybSjEnaWCr"},"source":["Define data path and define individual path for every files"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1670279261158,"user":{"displayName":"Jean Simonnet","userId":"17244487804686370097"},"user_tz":-60},"id":"_2E31PtNiozA"},"outputs":[],"source":["# write full filepath to list\n","def full_filepath_to_list(dataset_folder, file_limit=1000):\n","  dataset_files = os.listdir(dataset_folder)\n","  dataset_files = [f\"{dataset_folder}/{file}\" for count, file in enumerate(dataset_files) if count < file_limit]\n","  print(dataset_files)\n","  return dataset_files\n","\n","if not load_tokenized_datasets:\n","  dataset_path = encoded_midi_path\n","  dataset_path_all = os.path.join(dataset_path, \"all\")\n","  dataset_path_train = os.path.join(dataset_path, \"train\")\n","  dataset_path_valid = os.path.join(dataset_path, \"valid\")\n","  files_train = full_filepath_to_list(dataset_path_train) # training set\n","  files_valid = full_filepath_to_list(dataset_path_valid) # validation set"]},{"cell_type":"markdown","metadata":{"id":"xTrE4x-xLoAC"},"source":["## Put data into dictionnary"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1670279261159,"user":{"displayName":"Jean Simonnet","userId":"17244487804686370097"},"user_tz":-60},"id":"ReqyZyP1GrqG"},"outputs":[],"source":["# TO CHANGE  - this will probably give us some memory issues \n","# Dictionnary stage to skip here and load the date into the tokenizer directly from the text files\n","# show the first 2 items\n","def show_n_first_dataset_entries(dataset, n=2):\n","  for idx, key in enumerate(dataset.values()):\n","    if idx >= n:\n","      break\n","    else:\n","      print(key)\n","\n","def dataset_in_dictionary(files):\n","  dataset_dict = {}\n","  dataset_size = 0\n","  for _ , file in enumerate(files):\n","    line_count = 0\n","\n","    filename = file.split(\"/\")[-1].split(\".\")[0]\n","    file_content = open(file, \"r\")\n","\n","    while True:\n","      line_count += 1\n","      line =  file_content.readline().rstrip(\"\\n\") \n","      dataset_size += 1\n","      if line_count % 10000 == 0:\n","        print(f\"Writting - {filename} - count={line_count}\")\n","        print(line)  \n","\n","      if not line: # break when file is over\n","        print(f\"Breaking - {filename} - count={line_count}\")\n","        print(line)\n","        \n","        break\n","\n","      dataset_dict[f\"{filename}_{line_count}\"] = line\n","\n","    \n","\n","  show_n_first_dataset_entries(dataset_dict, n=2)\n","\n","  return dataset_dict, dataset_size\n","\n","if not load_tokenized_datasets:\n","  print(\"=============\")  \n","  print(\"Training data\")\n","  print(\"=============\")\n","  dataset_train_dict, dataset_train_size = dataset_in_dictionary(files_train)\n","  print(f\"dataset_train_size = {dataset_train_size}\")\n","  print(\"===============\")\n","  print(\"Validation data\")\n","  print(\"===============\")\n","  dataset_valid_dict, dataset_valid_size = dataset_in_dictionary(files_valid)\n","  print(f\"dataset_valid_size = {dataset_valid_size}\")\n","  # print(f\"dataset length: {len(dataset_train_dict.items()}\")"]},{"cell_type":"markdown","metadata":{"id":"JKuQBcHO8DuY"},"source":["# Tokenizer\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qFtBxLkIsAmr"},"source":["## Train the TOKENIZER"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1670279261160,"user":{"displayName":"Jean Simonnet","userId":"17244487804686370097"},"user_tz":-60},"id":"FbMJtqPJkvJx"},"outputs":[],"source":["from tokenizers import Tokenizer\n","from tokenizers.models import WordLevel\n","from tokenizers.pre_tokenizers import WhitespaceSplit\n","from tokenizers.trainers import WordLevelTrainer\n","from transformers import PreTrainedTokenizerFast\n","\n","\n","# making an iterator to go through the dictionary \n","# CHANG THIS TO LOAD DIRECTLY FROM THE FILES TO SAVE MEMORY\n","def get_training_corpus():\n","    for song in dataset_train_dict.values():\n","        yield song\n","\n","def define_and_save_tokenizer(\n","  saved_tokenizer=\"tokenizer.json\", \n","  tokenizer_path=\"./\"\n","):\n","  # using a Word Level Tokemizer\n","  tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n","  # separating vocabulary on whitespaces (pretokenizer)\n","  tokenizer.pre_tokenizer = WhitespaceSplit()\n","  # set up the trainer \n","  trainer = WordLevelTrainer(\n","      special_tokens=[\"[UNK]\", \"[PAD]\", \"[MASK]\"]\n","  )\n","  # special tokens I took the ones tristan used but I am not sure I understand all of them \n","  # [UNK] always need and UNK token\n","  # [PAD] if we need to do some pading\n","  # [MASK] if when we need to mask, in order to predict the future without seeing it\n","  # [CLS] what is it ? it is for BERT - the last token of the sequence\n","  # [SEP] what is it ? - SEP can be used with very short sequences\n","\n","  # get corpus\n","  training_corpus = get_training_corpus()\n","  # train tokenizer\n","  tokenizer.train_from_iterator(training_corpus, trainer=trainer)\n","  # save tokenizer\n","  tokenizer.save(f\"{tokenizer_path}/{saved_tokenizer}\")\n","\n","  return tokenizer\n","\n","if not load_tokenizer_pretrained:\n","  tokenizer = define_and_save_tokenizer(tokenizer_path=tokenizer_path)\n"]},{"cell_type":"markdown","metadata":{"id":"2Z_4vy6cPUz7"},"source":["## Or load the trained tokenizer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":388,"status":"ok","timestamp":1670279262191,"user":{"displayName":"Jean Simonnet","userId":"17244487804686370097"},"user_tz":-60},"id":"EVfS9F3HPTfx"},"outputs":[],"source":["if load_tokenizer_pretrained:\n","  from transformers import PreTrainedTokenizerFast\n","  # if device == \"cuda\":\n","  #   tokenizer = PreTrainedTokenizerFast(tokenizer_file=f\"{tokenizer_path}/{tokenizer_file}\")\n","  # else:\n","    # tokenizer = PreTrainedTokenizerFast(tokenizer_file=f\"{tokenizer_path}/{tokenizer_file}\")\n","  tokenizer = PreTrainedTokenizerFast(tokenizer_file=f\"{tokenizer_path}/{tokenizer_file}\")"]},{"cell_type":"markdown","metadata":{"id":"0-hYSVhWRaCm"},"source":[" ## Make it ready for GPT2 : add the pad_token"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1670279262192,"user":{"displayName":"Jean Simonnet","userId":"17244487804686370097"},"user_tz":-60},"id":"xEyb48_HRQfb"},"outputs":[],"source":["def format_tokenizer_for_transformers_classes(saved_tokenizer=\"tokenizer.json\"):\n","  # PreTrainedTokenizerFast makes the tokenizer usable by the transformer\n","  tokenizer = PreTrainedTokenizerFast(tokenizer_file = saved_tokenizer)\n","  tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","\n","  return tokenizer\n","\n","tokenizer = format_tokenizer_for_transformers_classes(\n","    saved_tokenizer=f\"{tokenizer_path}/{tokenizer_file}\"\n",")"]},{"cell_type":"markdown","metadata":{"id":"fBW_Rgm8QA_N"},"source":["## Visualize the Vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1670279262192,"user":{"displayName":"Jean Simonnet","userId":"17244487804686370097"},"user_tz":-60},"id":"g5r-usDzffae","outputId":"8aba5af8-9f8e-4c22-ef36-7ba6329e5860"},"outputs":[],"source":["print(tokenizer.vocab_size)\n","tokenizer.get_vocab()"]},{"cell_type":"markdown","metadata":{"id":"zEbAC-NKst-b"},"source":["## Tokenize the DATASET and prepare it for the model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1670279262194,"user":{"displayName":"Jean Simonnet","userId":"17244487804686370097"},"user_tz":-60},"id":"xBOS6W4ysx0H"},"outputs":[],"source":["def tokenize_function(data_to_tokenize):\n","  # this make the tokenized data ready for the model that requires a\n","    tokenized_data = tokenizer(\n","        data_to_tokenize,\n","        truncation=True,\n","        padding=True,\n","        max_length=1024,\n","    )\n","    return {\n","        \"input_ids\": tokenized_data[\"input_ids\"]\n","    } \n","\n","def tokenize_dataset(dataset_dictionary, tokenizer, limit=None):\n","  if limit is None:\n","    limit = len(dataset_dictionary.items())\n","\n","  tokenized_dictionary=[]\n","  for idx, file in enumerate(dataset_dictionary.values()):\n","    # print(file)\n","    if idx<limit:\n","      tokenized_dictionary.append(tokenize_function(file))\n","      # print(encoded_file)\n","\n","  return tokenized_dictionary\n","\n","def write_to_json(file, path):\n","  with open(f\"{path}\", 'w') as json_file:\n","    json.dump(file, json_file)\n","\n","def load_json(path):\n","  with open(f\"{path}\", 'r') as json_file:\n","    file = json.load(json_file)\n","    return file\n","\n","if not load_tokenized_datasets:\n","  dataset_train_tokenized = tokenize_dataset(dataset_train_dict,tokenizer)\n","  dataset_val_tokenized = tokenize_dataset(dataset_valid_dict,tokenizer)\n","\n","  write_to_json(dataset_val_tokenized, f\"{model_path}/dataset_val_tokenized.json\")\n","  write_to_json(dataset_train_tokenized, f\"{model_path}/dataset_train_tokenized.json\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":32554,"status":"ok","timestamp":1670279341742,"user":{"displayName":"Jean Simonnet","userId":"17244487804686370097"},"user_tz":-60},"id":"gCGHucujvgjq"},"outputs":[],"source":["if load_tokenized_datasets:\n","  dataset_train_tokenized = load_json(f\"{model_path}/dataset_train_tokenized.json\")\n","  dataset_val_tokenized = load_json(f\"{model_path}/dataset_val_tokenized.json\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1670279341743,"user":{"displayName":"Jean Simonnet","userId":"17244487804686370097"},"user_tz":-60},"id":"djrL5XXDkQKD","outputId":"40f82ee9-aa4a-4d84-8881-8b72877a287f"},"outputs":[],"source":["# checking that the data is in the correct shape to be input to the model\n","assert list(dataset_train_tokenized[0]) == [\"input_ids\"], list(dataset_train_tokenized[0])\n","print(type(dataset_train_tokenized[0]))\n","# Check a few samples\n","for i, ii in enumerate(dataset_train_tokenized):\n","  if i > 4: # print only the first 5\n","    break\n","  else: \n","    print(\"----\")\n","    # print(ii)\n","    print(dataset_train_tokenized[i])"]},{"cell_type":"markdown","metadata":{"id":"bAlPYX9JcGad"},"source":["# Define the model"]},{"cell_type":"markdown","metadata":{"id":"dEMfCqBWNY6n"},"source":["## Data collator.\n","[Huggingface](https://huggingface.co/transformers/v4.8.1/main_classes/data_collator.html)\n","\n","\"\n","Data collators are objects that will form a batch by using a list of dataset elements as input. These elements are of the same type as the elements of train_dataset or eval_dataset.\n","\n","To be able to build batches, data collators may apply some processing (like padding). Some of them (like DataCollatorForLanguageModeling) also apply some random data augmentation (like random masking) oin the formed batch.\n","\"\n","\n","\"\n","**class transformers.data.data_collator.DataCollatorForLanguageModeling**\n","- tokenizer (PreTrainedTokenizer or PreTrainedTokenizerFast) – The tokenizer used for encoding the data.\n","- mlm (bool, optional, defaults to True) – Whether or not to use masked language modeling. If set to False, the labels are the same as the inputs with the padding tokens ignored (by setting them to -100). Otherwise, the labels are -100 for non-masked tokens and the value to predict for the masked token\""]},{"cell_type":"markdown","metadata":{"id":"CA1YjYVuM_o6"},"source":["## Predicting the next note using GPT2\n","[HuggingFace](https://huggingface.co/transformers/v4.8.1/task_summary.html#)\n","\n","[Class GPT2Config](https://huggingface.co/transformers/v4.8.1/_modules/transformers/configuration_gpt2.html)\n","This is the config that needs to be passed to GPT2Model Classes.\n","\n","[Class GPT2LMHeadModel](https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2LMHeadModel)\n","Here we use the GPT2 Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1445,"status":"ok","timestamp":1670279343177,"user":{"displayName":"Jean Simonnet","userId":"17244487804686370097"},"user_tz":-60},"id":"HkH8zyZKc_M5","outputId":"44f80965-8acc-433e-8a6b-2fab7aec9718"},"outputs":[],"source":["# data collator\n","from transformers import DataCollatorForLanguageModeling\n","data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n","\n","# model\n","from transformers import GPT2Config, GPT2LMHeadModel\n","model_config = GPT2Config(\n","    vocab_size=tokenizer.vocab_size,\n","    pad_token_id=tokenizer.pad_token_id,\n","    n_embd=512, #Dimensionality of the embeddings and hidden states\n","    n_head=8,#Number of attention heads for each attention layer in the Transformer encoder\n","    n_layer=8, #Number of hidden layers in the Transformer encoder\n","    n_positions=1024, # maximum sequence length default = 1024\n",")\n","model = GPT2LMHeadModel(model_config)\n","model"]},{"cell_type":"markdown","metadata":{"id":"hy7gARUnxNdB"},"source":["# Testing the-jam-machine\n","Tristan's script"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"aborted","timestamp":1670279305221,"user":{"displayName":"Jean Simonnet","userId":"17244487804686370097"},"user_tz":-60},"id":"g1AJRUdbxQmU"},"outputs":[],"source":["import random\n","import matplotlib.pyplot as plt\n","import torch\n","\n","inputs = [random.choice(dataset_train_tokenized)]\n","inputs = data_collator(inputs)\n","assert list(inputs.keys()) == [\"input_ids\", \"attention_mask\", \"labels\"], list(inputs.keys())\n","print(\"input_ids:\", inputs[\"input_ids\"])\n","print(\"\")\n","\n","outputs = model(**inputs)\n","assert list(outputs.keys()) == [\"loss\", \"logits\", \"past_key_values\"], list(outputs.keys())\n","print(\"logits:\", outputs[\"logits\"])\n","\n","plt.plot(outputs[\"logits\"].detach().numpy()[0][0])\n","plt.title(\"Logits\")\n","plt.show()\n","plt.close()\n","\n","activations = torch.nn.functional.softmax(outputs[\"logits\"], dim=-1)\n","plt.plot(activations.detach().numpy()[0][0])\n","plt.title(\"Activations\")\n","plt.show()\n","plt.close()"]},{"cell_type":"markdown","metadata":{"id":"aLcy5V29cJ5R"},"source":["# Train the-jam-machine"]},{"cell_type":"markdown","metadata":{"id":"90rEyoJR-1Yi"},"source":["## Initialize Weight and Biases"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":201},"executionInfo":{"elapsed":15586,"status":"ok","timestamp":1670279358756,"user":{"displayName":"Jean Simonnet","userId":"17244487804686370097"},"user_tz":-60},"id":"NRoqheCiA4mQ","outputId":"3b4f5e9f-5493-48c0-9e7b-cee81243241e"},"outputs":[],"source":["# Weight and Biases\n","import wandb\n","os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"the-jam-machine_2022_12_04\"\n","wandb.init(project=\"the-jam-machine-test\")"]},{"cell_type":"markdown","metadata":{"id":"yVPBXOXR-XB4"},"source":["## Training"]},{"cell_type":"markdown","metadata":{"id":"pH99bb2cABm_"},"source":["Check those links (Hugging Face): \n","- [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)\n","- [trainer_utils.py](https://github.dev/huggingface/transformers/blob/main/src/transformers/trainer_utils.py)\n","\n","- [TrainingArguments (training_args.py)](https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1670279399587,"user":{"displayName":"Jean Simonnet","userId":"17244487804686370097"},"user_tz":-60},"id":"gTaBNeOCCYBa","outputId":"8c42ec14-a140-442f-be83-23e3f52a8990"},"outputs":[],"source":["checkpoint_path"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":827,"status":"ok","timestamp":1670279403786,"user":{"displayName":"Jean Simonnet","userId":"17244487804686370097"},"user_tz":-60},"id":"IbNpaa7PjcsJ"},"outputs":[],"source":["# defining training params\n","prop_dataset_to_pass_before_evaluating = 1/10\n","data_samples_to_pass_before_evaluating = prop_dataset_to_pass_before_evaluating*len(dataset_train_tokenized)\n","per_device_train_batch_size = 8\n","device_nb = 1\n","num_train_epochs = 2\n","eval_steps = round(data_samples_to_pass_before_evaluating / num_train_epochs / per_device_train_batch_size / device_nb)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YHLjkDERcMIG"},"outputs":[],"source":["from transformers import TrainingArguments, Trainer\n","import os\n","# Create the trainer.\n","print(\"Creating trainer...\")\n","training_args = TrainingArguments(\n","    output_dir=model_path, # where model and checkpoints will be saved\n","    overwrite_output_dir=True,# self-explanatory\n","    num_train_epochs=num_train_epochs, # defaults to 3.0\n","    evaluation_strategy=\"steps\", # Evaluation is done (and logged) every eval_steps (so not at the end of every epoch)\n","    eval_steps=eval_steps, # Number of update steps (backprob) between two evaluations\n","    learning_rate=5e-4, #initial learning rate for :class:`~transformers.AdamW` optimizer.\n","    weight_decay=0.15, # Default o.\n","    warmup_steps=5_000,\n","    lr_scheduler_type=\"cosine\", # Default linear\n","    fp16=False, # Default False. Is True in HF.\n","    bf16_full_eval=False,\n","    per_device_train_batch_size=per_device_train_batch_size, #default value is 8\n","    seed=42, # default seed=42\n","    save_strategy=\"steps\", # \n","    save_steps=eval_steps*2,\n","    save_total_limit=5,\n","    logging_strategy=\"steps\",\n","    logging_first_step=True,\n","    logging_steps=eval_steps,\n","    logging_dir=os.path.join(model_path, \"logs\"),\n","    prediction_loss_only=False,\n","    report_to=\"wandb\",\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"loss\", # \"eval_loss\"\n","    push_to_hub = True,\n","    hub_strategy=\"end\", #huggingface repo -> saves the final model at the end of training\n","    hub_model_id=hf_repo,\n","    hub_token=hf_token,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=dataset_train_tokenized,\n","    eval_dataset=dataset_val_tokenized,\n",")\n","\n","# Save the tokenizer.\n","tokenizer.save_pretrained(model_path)\n","\n","# Train the model.\n","if train_from_scratch:\n","  trainer.train()\n","else:\n","  if train_from_this_checkpoint is not None:\n","    trainer.args.num_train_epochs += additionnal_epochs_to_run\n","    trainer.resume_from_checkpoint = checkpoint_path # that is not used by the trainer but potentially for the metrics # to check\n","    trainer.train(checkpoint_path)\n","\n","  else:\n","    pass\n","# Save the model.\n","model.save_pretrained(model_path)\n","# Push to Hugging Face\n","trainer.push_to_hub(commit_message = \"End of training\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["I3vAEjWWI2On","5gwWdCMCeLPS","dEMfCqBWNY6n","9Qsez4D--lM_"],"machine_shape":"hm","provenance":[{"file_id":"1P6g_69KPa6SkOzt5wrGf3PBdrSzs3Rfy","timestamp":1669976977786},{"file_id":"1HFBOnIhuG9GQldMx8mzRqigdfjGlyA8q","timestamp":1669194133466},{"file_id":"1PowSw3doBURwLE-OTCiWkO8HVbS5paRb","timestamp":1667242191762},{"file_id":"1-w1eLEUMdTcFyOJAYC5SgTVH7pR3aTrr","timestamp":1666684778954}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.7 ('the-jam-machine-C486_rTm')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.7"},"vscode":{"interpreter":{"hash":"05f180633b6db30f1627d6c6525b2893a825dbfd9b5c1cd1f75c024f33da1787"}}},"nbformat":4,"nbformat_minor":0}
